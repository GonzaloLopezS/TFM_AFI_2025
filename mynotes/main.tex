\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{Transporte marítimo e incautación de droga}
\author{Gonzalo Lopez Segovia}
\date{December 2024}

\begin{document}

\maketitle

\section{Resumen}

\section{Abstract}

	\subsubsection{Palabras clave}
	Transporte marítimo, Geografía del Transporte, Geografía Marítima, Drogas, Estupefacientes, Narcotráfico, Machine Learning, Análisis de Datos, Clustering.

\section{Introducción}

	\subsection{Antecedentes}
	Importancia del transporte marítimo en el comercio global y su relación con el tráfico de drogas.
	Otros proyectos.

	\subsection{Motivación del proyecto}
	% Descripción del problema

	\subsection{Objetivos}
	De acuerdo con los expuesto en la sección "Motivación del proyecto", este trabajo se va a encargar de:

	1. Agrupar puertos en regiones geográficas y metropolitanas próximas denominadas "hubs portuarios" a partir de señales de comunicaciones enviadas desde barcos próximos.

	2. Mostrar el comportamiento de distintos puertos con respecto al volumen de droga incautada por tipo de droga.

	3. Estudiar si el comportamiento puede ser caracterizado por factores periódicos en forma de serie temporal para un intervalo de 75 meses (6 años y 1 trimestre).
	
	4. Determinar si a partir de la actividad en un puerto comercial, dado volumen de carga y descarga en ese puerto, se podría estimar la cantidad de droga incautada en ese mismo puerto.

	# Conocimientos: Geografía del Transporte, Geografía Marítima, Estadística Descriptiva, Series Temporales, Clustering, Visualización-

	\subsection{Enfoque y Metodología general}

	\subsection{Estructura del Documento}


\section{Estado del arte}

Al faltar datos de flujos y vías marítimas, no es trivial hallar la red o grafo mundial de conexiones portuarias.

	\subsection{Literatura sobre el problema}
	
		\subsubsection{Estudio de la Geografía del Transporte.}
		
		
		\subsubsection{Estudio sobre dinámicas del narcotráfico}


	\subsection{Conocimiento de técnicas de Machine Learning}
	Métodos tradicionales vs Métodos basados en Machine Learning/Data Science.
	Modelos y técnicas más utilizados en trabajos previos


		\subsubsection{Series temporales en python: Prophet.}
		La falta de datos a nivel temporal (poco rango y poca resolución) hizo que pronto se desistiese por la implementación de un modelo basado en series temporales.

		\subsubsection{Técnicas de clustering.}
		-- Incluir Vectorial K-Means (*)\
		Agrupamiento de los barcos de EE UU mediante técnicas de clustering. Objetivo: hallar punto en el litoral que sirva como "punto de puerto" para los barcos más próximos. Es preferible que se trate de un algoritmo de clustering de tipo jerárquico: para ir aumentando la granularidad basándose en la situación geográfica.
		Algoritmos tipo K-Means o PAM quedan descartados por dos motivos: no son jerárquicos y, sobre todo, su centro de masas tenderá a ser escogido en alta mar (o como punto medio o mediano de los barcos que formen cada clúster).\
		
	\subsection{Justificación de las herramientas y tecnologías elegidas}
	
	
\section{Metodología empleada}	
%Conjunto de métodos y técnicas aplicadas durante un proceso de investigación para alcanzar resultados teóricamente válidos.
%Sirve como guía de los procedimientos de la investigación

	\subsection{Descripción del conjunto de datos utilizado}
	Para la puesta a punto del conjunto de datos a tratar, primero hubo que implementar un desarrollo de integración de los mismos provenientes de distintas fuentes.\
	Para ello, hubo que conseguir un compromiso entre aquellos datos obtenidos del transporte de mercancías en puertos marítimos con los datos oficiales sobre incautaciones. La clave para ello estuvo en estandarizar el intervalo y el nivel de detalle en el eje temporal (meses para los últimos seis años fiscales en EE UU) y en el nivel geográfico.\
	
		\subsubsection{Origen de los datos}
		%Primera aproximación al problema
		Originalmente, el objetivo del trabajo buscaba relacionar el comercio global y sus flujos regionales con el tráfico de drogas. Debido a las complejidades a la hora de acceder a estos datos y que estos dependen de agencias privadas (bajo licencia), se replanteó el problema de tal modo que se trabajasen sobre puertos destino en Estados Unidos (datos abiertos) y no sobre un modelo de grafos basado en las rutas marítimas de transporte. La ventaja de los datos provistos por Estados Unidos (ya sea a nivel federal, nivel estatal o local) es que éstos suelen ser de buena calidad.\
		
		%Fuentes, Bases de Datos, Informes...
		Los datos de transporte de contenedores se obtuvieron de las distintas páginas webs informativas de cada autoridad portuaria. Accediendo a las secciones de informes estadísticos se pudieron obtener datos a nivel mensual sobre carga y descarga de contenedores, así como exportaciones e importaciones. Todos aquellos puertos de EE UU que proveyeron esta información fueron considerados objeto de estudio. Por otro lado, los datos referidos a incautaciones de drogas se obtuvieron los informes estadísticos de incautación de droga provistos por la CBP (Agencia de aduanas de Estados Unidos) para las distintas oficinas regionales a nivel mensual para los años fiscales 2019 a 2024.\
		
		Una vez se localizaron las fuentes y los datos de interés, se procedió a su descarga. La construcción del dataset de entrada se trató de un problema de integración de datos. Se usó tanto la fecha (en formato "Año-mes") como la región geográfica como clave primaria para unir los datos de trasiego de contenedores en un puerto como de incautaciones de drogas para oficinas del CBP asignadas para su hub portuario correspondiente. Ya que a nivel temporal, el nivel de detalle que se pudo conseguir fue a nivel mensual, se obtuvieron de partida setenta y cinco meses de observaciones (equivalentes a seis años más un trimestre debido a que los años fiscales en Estados Unidos comienzan el 1 de octubre del año natural anterior).\
		
		Dado que el CBP se trata de una agencia federal de Estados Unidos,se evitó el inconveniente que pudiera acarrear si cada estado tuviese su agencia propia y no existiese una estandarización de los procedimientos y oficinas. Todos aquellos puertos que ofrecieron datos de calidad en relación a operaciones sobre contenedores (total de TEUs, Importaciones Totales, Exportaciones Totales, Contenedores Vacíos y Contenedores Llenos) y que resultasen de interés para el trabajo, fueron incluidos.\
		
		%Detección de hubs portuarios: Clustering.
		Como se mencionó en la sub-sección \cite[Técnicas de clustering]{keylist}, originalmente, para simplificar el número de puertos y poder asignarlos a las oficinas del control de aduanas de Estados Unidos, se procedió a usar técnicas de Clustering (K-Means) para poder estimar en qué regiones geográficas se podían agrupar aquellos puertos que fueran cercanos geográficamente entre sí. Pese a ello, dadas las circunstancias y adversidades durante el desarrollo del trabajo, se simplificó bastante a la hora de integrar los datos. El resultado fue la asignación de coordenadas esféricas a partir de las señales emitidas por los barcos mercantes y detectadas por la agencia meteorológica de EE UU (NOAA, por sus siglas) durante quince días del mes de septiembre de 2024. Estas coordenadas sirvieron para estimar de forma aproximada barcos atracados en puertos próximos entre sí y, en último término, asignarlas a oficinas del CBP. Gracias al conocimiento geográfico de Estados Unidos, no fue demasiado complicado asignar los hubs portuarios a las agencias aduaneras.\
		
		%Asignación de un hub portuario con una agencia aduanera.
		Tras la generación de la variable 'Hub Portuario' que pudiera agrupar puertos cercanos entre sí (caso más paradigmático, el caso de estudio del Área Metropolitana del Gran Los Ángeles agrupando los puertos de Los Ángeles y Long Beach), se les asignó a cada oficina aduanera correspondiente del CBP. Pese a que en las fuentes originales se dice que en el caso de los datasets referidos a "Nationwide Drug Seizures: Los agentes y oficiales del CBP prohiben la entrada y salida de productos narcóticos ilícitos a través de sus fronteras y en los puertos de entrada" \cite[CBP Nationwide]{}.\ Esto quiere decir, por un lado, que son datos centrados en parte a incautaciones en puertos -útil para el planteamiento del problema- frente a otras fuentes de datasets del mismo CBP que hacían referencia a AMO (Operaciones marinas y aéreas y cuyo alcance o jurisdicción quedaba, según la fuente original, más laxa* \cite[]{}) y por el otro, que posiblemente no se ciñiesen exclusivamente a éstos.\
		
		Por ejemplo, tras todo lo comentado en el anterior párrafo, hubiese sido interesante de incluir los datos de una ciudad fronteriza como San Diego. Sin embargo, no se lograron encontrar datos de suficiente calidad que pudieran ser integrados y provistos por la autoridad portuaria del puerto de San Diego \cite[San Diego Unified Port]{}. Sin embargo, si se consiguió para los puertos de Seattle y Tacoma \cite[]{}, en la frontera con Canadá.\
		
		%Pivote de las variables sobre incautaciones de drogas:
		Por su parte, los datos provenientes de los datasets sobre incautaciones por parte de los "Field Office" (u oficinas de campo/regionales) tuvieron que pasar de una estructura "table longer" a una "table wider" para así poder encajar mes a mes con los datos de trasiego de contenedores. Además, pivotando respecto al tipo de drogas, se logró generar todas las variables de conteo de redadas y de cantidad incautada para cada tipo de droga. Generando así un dataset más rico en término de variables.\
		
		% Generación de la variable 'Sum of Counts'
		Tras haber realizado el pivote de los datos de incautaciones de drogas para poder saber la cantidad y el número de drogas por cada tipo de droga y para cada mes, se generó una variable "Sum of Counts", la cual será desarrollada en la subsección \cite[Feature Engineering]{keylist}. La motivación en este caso, se fundamentó en la necesidad de agrupar un dato global y la inviabilidad de hacerlo sumando las cantidades (sea en kilogramos o libras) para tipos de drogas tan diferentes entre sí, junto con otros aspectos: el valor estimado, el nivel de adicción, letalidad...
		 
		% El dataset construido
		Tras haber estudiado las fuentes originales, haber operado de modo que se pudieran compatibilizar los dos distintos ámbitos de estudio (el análisis del tráfico marítimo con la incautación de drogas) y la integración de los mismos en sentido último, se fueron generando, en primer lugar, conjuntos de datos para cada hub portuario y finalmente un conjunto de datos que integraba los distintos hubs portuarios en uno mismo.\cite[Puertos en el análisis]{} \
		
		A nivel técnico, esta integración entre distintas fuentes se realizó mediante operaciones "merge" proporcionadas por la librería de pandas para poder cuadrar los datos de distinta índole.\
		
		% Fort-Lauderdale tuvo un comportamiento particular
		Un caso particular en la integración de datos se correspondió con los extraídos del Puerto de Everglades en Fort-Lauderdale (Área Metropolitana de Miami, Florida). Se reseña que no se pudieron incluir los datos del puerto de Miami por falta de calidad de los mismos (falta de granularidad, acuerdos de proceso de contenedores con operadores privados en los terminales que podían dificultar la integración de los datos \cite[]{}). Los datos fueron extraídos de los informes para cada año fiscal a disposición en \cite[]{}. En algún caso particular, se observó la ausencia de datos para el año fiscal 2021 (últimos tres meses). El tratamiento de estos missing values será desarrollado en su sección correspondiente.
		
		% Esquema dibujo de la construcción del dataset integrado.
		Una vez conformado el dataset, se realizó un primer análisis exploratorio de datos. Por un lado, se realizó uno para cada hub portuario gracias al manejo de herramientas y librerías de visualización de python como plotnine \cite[plotnine]{} o ydata-profiling \cite[ydata-profiling]{}. Por otro lado, también se estimó la importancia que pudieran tener las variables geográficas. En este último caso, se generaron mapas geográficos con la librería plotly \cite[plotly]{}
		
		% Análisis Exploratorio de los datos
		
		
		% Matriz de correlación de datos
		
		\subsubsection{Pre-procesamiento de los datos}
		% Limpieza, Tratamiento de Missing Values, Normalización, Estandarización, Tratamiento de Outliers...
		Este primer dataset construido, consta de 31 variables: Date (formato fecha 'YYYY-mm'), latitud y longitud (numérico float64), siete variables numéricas correspondientes a los datos portuarios, el resto de variables corresponden a los datos de incautaciones de drogas: diez variables numérica sobre "número de eventos" por droga o "Count of Events" en la versión original más una variable sumatoria de estas diez "Sum of Counts" y otras diez sobre la cantidad (en libras) incautada por droga.\
		
		% Aplicaciones de scikit-learn
		A la vista del primer análisis exploratorio de datos, se observaron missing values en las variables relacionadas con las incautaciones de drogas y una gran diferencia de escala entre los datos portuarios (centenas de miles de contenedores) frente a los datos de incautaciones (decenas de incautaciones). Los missing values, al tratarse de datos de fuentes de origen fiables, se imputaron a cero haciendo uso de la función SimpleImputer provista por la librería de scikit-learn. Se consideró que, aquel mes para aquella autoridad aduanera, no hubo incautación. Para seguir por esa línea, se observó una constancia en la ausencia de datos en los pares número de incautaciones-cantidad incautada. Es decir, se comprobó que si no había datos de incautación, no había datos de cantidad incautada y, por tanto, se imputase a cero. Por su parte, para la normalización de datos se aplicó la función StandardScaler también proporcionada por scikit-learn. Es a partir de aquí, que los datos con los que se van a trabajar están estandarizados a media cero y varianza uno.\
		
		% Detalles del pipeline de scikit-learn.
		Dado que los rangos para variables de distinto tipo (incautaciones vs contenedores) eran muy diferentes entre sí, se procedió a la estandarización de los mismos mediante la implementación de pipelines en scikit-learn. Los datos numéricos ausentes se imputaron a cero (como se explicó en el párrafo anterior) y se estandarizó a una normal(0,1). Por su parte, las variables categóricas se dispusieron a ser tratadas mediante 'OneHotEncoder'*.\
		
		% Error original en los datos de Los Angeles:
		En el caso de los datos obtenidos del puerto de Los Ángeles, se descubrió un error de formato en los datos de origen. En efecto, para el dato del mes de noviembre del 2020 (año fiscal 2021) se descubrió un error de formato de cambio de separador de miles (coma vs punto) en la columna 'Total TEUs'. Dado que se verificó que esta columna era igual a la suma de los valores de las columnas 'Total Imports' and 'Total Exports', se aplicó la suma de esos valores para la observación del mes de noviembre de 2020.
		
		% Datos en miles para el puerto de Newark.
		Por su parte, resultó que los datos obtenidos para el puerto de Newark (Área Metropolitana de Nueva York-Nueva Jersey) estaban contados en miles (de contenedores). Para resolverlo, se multiplicaron estos valores por mil aparte del resto de datos para después, ser reintegrados \cite[]{}. Posiblemente el problema de integración se tratase de una incompatibilidad entre el formato europeo y americano de separador de miles.
		
		% Ensamblaje y Missing Values en los datos de Fort Lauderdale
		Por su parte, como se mencionó en la sección \cite[Origen de los datos]{}, en el caso de Fort Lauderdale, debido a las características particulares de su adquisición e integración de datos, se observaron la falta de los mismos correspondientes al último trimestre del año fiscal 2021.\
		
		Una ventaja que se tenía a la hora de poder imputar estos datos, era la posesión de todos los datos de años fiscales a nivel anual, es decir, el valor integrado de los datos mensuales \cite[]{Preliminary_Waterborne_Commerce_Chart_2024}. Conociendo estos datos y dado que no solo ofrecían el total de contenedores o de importaciones/exportaciones sino también qué contenedores están cargados y cuáles están vacíos, se pudo plantear el problema de la imputación de los missing values como valores MNAR, es decir, imputarlos a partir del contexto. Se aseguró que realmente los datos anuales coincidían con los datos mensuales y, en consecuencia, se fueron imputando según variables: de Loaded Imports, a Empty Exports, así hasta el total de importaciones y de exportaciones y, en último lugar, el total de TEUs (suma de las dos variables previas). En último lugar, respecto a esta operación, se imputó el mismo valor para cada mes ausente (una alternativa que se planteó fue la imputación aleatoria con una media próxima al valor imputado pero una cierta varianza incluida).\
		
		% Tratamiento de Outliers
		Para el tratamiento de Outliers, en un primer momento se fueron estudiando caso a caso, la situación de cada uno, su contexto, etc. Intentando comprender qué valor aportaba al modelo, si pudiera ser un dato que alterase en exceso al resto o si fueran datos que simplemente estuviesen un poco más allá de las fronteras teóricas (rango intercuartil, QQ-Plot, método de las tres sigmas) fuera de lugar. Dado que el número de observaciones no era demasiado grande, en muchos casos, se limitó a que el umbral para excluir datos fuera el valor absoluto cinco de los datos normalizados. Es decir, se intentó minimizar el número de datos excluidos, pero se decidió que en algunos casos, era necesario para avanzar en el estudio del problema.
		
		% [Visualización Outlier de GLA]
		
		
		% Visualización (o datos) de media por hub portuario de incautaciones y de tráfico de contenedores
	
		\subsubsection{Ingeniería de características}
		Tras unas primeras visualizaciones y determinadas certezas respecto a los datos brutos que componían el dataset primitivo, se estimó necesario la inclusión de otras variables que fuesen generadas a partir de éstas primeras.\
		% Generación de predictores. Filtrado y suavizado. Obtención de la derivada.
		% Generación del target (y posibles alternativas)
		
		%Sum of Counts
		Uno de los puntos que más quebraderos de cabeza trajo respecto al desarrollo del modelo fue, por encima de todo, qué variable se usaría como target. Pese a que este punto será abordado con mayor profundidad en la sección \cite[Implementación Práctica del Modelo]{}, pronto surgió la idea de incluir una variable que agrupase todas las incautaciones (número de redadas) sin importar la categoría de la droga para un mismo mes y una misma región. Sin embargo, esto no era posible realizarlo sobre la cantidad de droga incautada (masa en libras) ya que carecía de sentido mezclar masas de distintos orígenes (se llegó a pensar en hacer la suma del valor incautado, pero también se descartó por la falta de datos al respecto y, que la influencia del mismo valor al igual que la masa generase ciertas distorsiones entre una región u otra).\ 
		
		%Ratios por Tipo de Droga
		Una vez generada la variable "Sum of Counts", era posible tener una asignación de cada puerto con la cantidad total de redadas mensuales. Pero dentro de esas redadas, se consideró que aportaría valor el conocer cómo eran esas redadas: si lograban continuamente capturar gran cantidad de estupefacientes, si en cambio, solían ser pequeñas incautaciones con algún "premio" en forma de gran redada... Si esto dependía del tipo de droga y/o del hub portuario. Para ello se fueron creando las variables "Ratio_" para cada tipo de droga que era el cociente entre la cantidad incautada de un tipo de droga durante un mes en una región y el número de redadas durante ese mes en esa misma región.\
		
		% Incluir cálculo "Ratio_"
		
		
		Como se mencionó en el párrafo anterior, con estas nuevas variables se buscaba ver a primera vista qué tipo de incautaciones se llevaban de forma regular para cada hub portuario. Por ejemplo, para los casos de incautaciones de cocaína, los puertos de Newark y de Miami eran constantes las redadas con importantes cantidades incautadas. Por otro lado, en Seattle/Tacoma se encontraron picos de incautación (similar a una delta) en algún mes concreto.\
		
		% Incluir gráficas del Ratio Variables:
		Para el cálculo de las variables de ratio, se barajó realizar el cálculo a partir de los datos normalizados. Sin embargo, a la vista de resultados contradictorios, se decidió finalmente realizarlo sobre los datos brutos. Una de las contraindicaciones que surgía de esta última forma de calcularlo era la aparición de indeterminaciones del tipo 0/0 fruto de observaciones con missing values. Dado que los missing values correspondían a meses estimados sin redadas y, por lo tanto, sin incautaciones, y que el resto de valores eran forzosamente enteros positivos, el valor del ratio debía ser definitivamente, entero positivo, estrictamente mayor que cero. Estas aseveraciones permitieron, sin lugar a dudas, imputar los ratios en los casos de missing values a cero. Su sentido dentro del contexto de los datos de esa variable es que el cero es el mínimo valor para esa variable y que ninguna redada realizada podía alcanzar ese mínimo. Por su parte, el descarte de realizar el cálculo a partir de los datos normalizados era la paradoja resultante de obtener un ratio negativo (por debajo de la media) en el caso de tener una cantidad incautada elevada (valor positivo por encima de la media, numerador del cociente) frente un número bajo de redadas (valor negativo por debajo de la media, denominador del cociente) que, en relación con el resto de ratios, debería estar en el extremo positivo de la distribución. Finalmente, al igual que el resto de variables, antes de ser integradas en el dataset fueron estandarizadas mediante StandardScaler() implementado en el Pipeline de scikit-learn.\
		
		
		
		% Suavizados de señales originales.
		% Derivadas sobre suavizados.
		%¿Por qué se realiza todo esto?: Si el volumen de carga de un puerto mantiene alguna relación con la cantidad de droga total incautada.
		
		%¿Por qué no se suman las cantidades de droga incautada?
		Pese a que hay elementos interesantes en favor de ello: la cantidad en kilogramos incautada de un tipo de droga A frente a otro tipo B no sería determinante a la hora de realizar un análisis y que podría inducir a error, quizá si lo fuese su equivalente en valor monetario -pero inviable en términos prácticos-.\
		
	\subsection{Estrategia de división de datos}
	% Train-Test Split, Validación Cruzada, GridSearch para estimación de hiperparámetros.
	

\section{Diseño del Modelo}
	\subsection{Algoritmos y técnicas de Machine Learning utilizados}
	% Detección de hubs portuarios: una primera incursión en modelos de Machine Learning.
	% No todos los puertos, a pesar de su volumen, tenían un emparejamiento claro con la agencia aduanera.
	% Hiperparámetros y técnicas de optimización.
	
	
	Como se vio en la sección \cite[]{}, hubo ciertas dificultades a la hora de plantear el target y, en definitiva, lo que quiere predecir el modelo, se han desarrollado dos pequeños modelos al respecto: uno para predecir la cantidad incautada por redada en Newark (Estados de Nueva Jersey y Nueva York) y otro sobre la cantidad de redadas con éxito(*) en Port Everglades (Área Metropolitana Miami-Fort Lauderdale).
	
	% Modelo para el puerto de Nueva York
	
	
	
	% Modelo para Miami/Fort Lauderdale

	\subsection{Implementación Práctica del Modelo}
 	- Descripción de las tareas realizadas para implementar el proyecto.
 	- Se destacan las principales actividades y decisiones.
 	- Fases de desarrollo del proyecto.
 	- Diagrama de Gantt con las tareas realizadas.



	\subsection{Evaluación del rendimiento}
	% Métrica utilizadas.
	% Comparaciones con alternativas.

\section{Interpretación de los resultados}

	\subsection{Visualización de resultados}
	
	%\subsection{Visualización de resultados}
	\subsection{Sesgos y limitaciones}


\section{Conclusiones y trabajo futuro}

	\subsection{Hallazgos relevantes}
	
	\subsection{Aplicabilidad del modelo en entornos reales}
	
	\subsection{Limitaciones del trabajo}
	
	\subsection{Líneas de investigación futuras}

\section{Bibliografía}

\section{Glosario}
Field Office
MNAR
TEU

\section{Anexos}
https://www.portoflosangeles.org/business/statistics/container-statistics/historical-teu-statistics-2020
https://www.cbp.gov/
https://www.cbp.gov/document/stats/nationwide-drug-seizures
https://www.cbp.gov/document/stats/amo-drug-seizures
https://www.portofsandiego.org/
https://www.nwseaportalliance.com/about-us/cargo-statistics
https://plotnine.org/
https://docs.profiling.ydata.ai/latest/
https://plotly.com/python/
https://www.miamidade.gov/portmiami/cargo.asp
https://southfloridacontainer.com/
http://pomtoc.com/
https://www.seaboardmarine.com/
https://assets.simpleviewinc.com/simpleview/image/upload/v1/clients/porteverglades/January_Monthly_Loaded_TEUs_02_13_2025_9e6f70c1-032a-4bbc-8f69-2d027dcfa394.pdf
https://assets.simpleviewinc.com/simpleview/image/upload/v1/clients/porteverglades/FY2023_September_TEUS_Loaded_by_Month_7065b619-cddd-4b97-a02c-8c247c743ac0.pdf
https://assets.simpleviewinc.com/simpleview/image/upload/v1/clients/porteverglades/August_TEUS_Loaded_by_Month_Fiscal_2020_Calander_2020_9439e3f7-7154-4030-a9b8-9ec592a57fde.pdf
https://assets.simpleviewinc.com/simpleview/image/upload/v1/clients/porteverglades/Preliminary_Waterborne_Commerce_Chart_2024__1a6b16e6-94b2-46ac-89e7-d7080779f346.pdf
https://www.panynj.gov/port/en/our-port/facts-and-figures.html

Puertos en el análisis: Los Angeles, Long Beach, Seattle, Tacoma, Everglades-Fort Lauderdale, Houston, Newark-New York, Oakland.

\end{document}

 